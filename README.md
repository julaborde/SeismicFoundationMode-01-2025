# Multimodal Seismic AI

This project explores the development of a multimodal AI model combining seismic waveforms and geophysical images, based on two foundation models: Seismic Foundation Model (SFM) and SeisLM.

## Features

- Training and evaluation of the SFM model
- Signal-to-image transformation using Mel spectrograms
- Implementation of a simple visualization tool
- Performance benchmarking using IoU and CPA
- Exploration of multimodal fusion using Meta's ImageBind

## Project context

This work was conducted at CentraleSupélec in collaboration with Dr. Filippo Gatti (LMPS Lab), aiming to improve seismic interpretation through multimodal AI integration.

## Team

Julien Laborde-Peyré, Enio Lahitte, Yuhan Ma, Timothée Colette

*January 2025*
